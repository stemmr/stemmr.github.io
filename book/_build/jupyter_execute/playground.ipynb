{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'Notes from the Latent Space'\n",
    "date: 2024-02-20T17:18:05+01:00\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *\"Talking nonsense is the sole privilege mankind possesses over the other organisms. It's by talking nonsense that one gets to the truth! I talk nonsense, therefore I am human.\"* \n",
    "> \n",
    "> -- Fyodor Dostoevsky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Vector Spaces](#Vector-Spaces)\n",
    "    1. [Basis](#basis)\n",
    "    2. [Span](#span)\n",
    "    3. [Subspaces](#subspaces)\n",
    "    4. [Linear Independence](#linear-independence)\n",
    "3. [Matrices](#Matrices)\n",
    "4. [Matrix Decompositions](#third-example)\n",
    "5. [Determinants](#determinants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Linear algebra is one of those topics that can at times be quite unintuitive. I find myself having to revisit topics from time to time, and while I would hope that they just stuck with me I sadly don't have a photographic memory. Here's an attempt at capturing any tidbit about Linear Algebra I might encounter such that I can quickly read through this blog to refresh my knowledge. First, let's set some things up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Spaces\n",
    "\n",
    "A vector space is a set of elements for which the following operations are defined, which we will call \"vector addition\" (which we can define as we please to form a vector space, and does not necessarily have to be standard addition):\n",
    "1. Closure under vector addition\n",
    "2. Commutation under vector addition $v + w = w + v$\n",
    "3. Associative under vector addition $v + (w + x) = (v + w) + x$\n",
    "4. There is a zero vector, such that $0 + v = v + 0$\n",
    "5. Each vector has an additive inverse, such that $v + w = 0$ for any $v$\n",
    "\n",
    "There must also be scalar addition such that:\n",
    "1. Distributive $k(v + w) = kv + kw$\n",
    "2. $(k_0 + k_1)v = k_{0}v + k_{1}v$\n",
    "\n",
    "The more general term for a vector space is a [Field](https://en.wikipedia.org/wiki/Field_(mathematics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basis\n",
    "A basis is a set of vectors such that every vector in the vector space can be expressed as a sum of the basis vectors. These vectors must Span a vector space `V` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Span\n",
    "The set of vectors `W` that can be formed from some other set of vectors `V` by a scaled sum of the vectors in `V` is called the span of `V`. Where a scaled sum is $k_0V_0 + k_1V_1 + ...$ in $W$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subspaces\n",
    "A linear subspace `S` of a vector space `V` is a subset of `V` such that the linear subspace remains closed under:\n",
    "1. Vector Addition. \n",
    "   - The vector addition operation for `V` applied on any vector in `S` must ensure that and vector in `S` remains in `S` \n",
    "2. Scalar Multiplication\n",
    "   - Any vector in `S` scalar multiplied with any scalar must remain in `S`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Independence\n",
    "A set of vectors $V = v_0, \\dots, v_n$ is said to be linearly independent if the only way to form the 0 vector is by picking 0 for all $a_0, \\dots, a_n$ in the equality $0 = a_0v_0 + \\dots + a_{n}v_n$\n",
    "\n",
    "In fact, for a vector space with dimension `n` a linearly independent list of vectors with length `n` will be a basis of the vector space, and linearly independent lists of length `< n` can be extended to be a basis of the vector space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Functions\n",
    "Linear Functions are function from one vector space to another vector space that preserve both properties of vector addition and scalar multiplication.\n",
    "1. `f(k0v0 + k1v1) = k0f(v0) + k1f(v1)`\n",
    "\n",
    "### Types of Mappings\n",
    "- Linear\n",
    "- Affine\n",
    "\n",
    "### Vectors Operations\n",
    "- Inner Product: $\\langle v , w\\rangle = (v_0, \\ldots, v_n) \\cdot (w_0, \\ldots, w_n) = v_0w_0 + \\cdots + v_nw_n$\n",
    "- Outer Product: $u \\otimes v = A$ where for vectors of size $m \\times 1$ and $n \\times 1$ the matrix A will be $m \\times n$ with $A_{ij} = u_iv_j$\n",
    "- Cross Product: $a \\times b = \\|a\\|\\|b\\|\\sin(\\theta)n$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices\n",
    "The most intuitive way to understand matrices is as a linear function over vectors from one domain into another domain, using the standard basis vectors. For example, we could rewrite matrix multiplication as below. NB! We could swap out the basis vectors to be different, which would then also redefine the definition of matrix multiplication. It is merely a strong convention to use $(0, 1)$ and $(1, 0)$ as bases.\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "   a & b \\newline\n",
    "   c & d\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "   x \\newline\n",
    "   y\n",
    "\\end{bmatrix}\n",
    " = (ax + by)\\begin{bmatrix}\n",
    "   1 \\newline\n",
    "   0\n",
    "\\end{bmatrix} + (cx + dy)\\begin{bmatrix}\n",
    "   0 \\newline\n",
    "   1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Types\n",
    "### 1. Zero Matrix\n",
    "### 2. Identity Matrix\n",
    "### 3. Jacobian Matrix\n",
    "The Jacobian Matrix van be computed on a vector-valued function (a function that takes vectors as in/outputs). It represents all the first-order partial derivatives on the function.  \n",
    "\n",
    "$$\n",
    "J = \\begin{bmatrix} \\frac{\\partial \\bold{f}}{\\partial x_1} & \\dots & \\frac{\\partial \\bold{f}}{\\partial x_n} \\end{bmatrix} =\n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\dots & \\frac{\\partial f_1}{\\partial x_n} \\newline\n",
    "\\vdots & \\ddots & \\vdots \\newline\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\dots & \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hessian Matrix\n",
    "The Hessian Matrix $\\bold{H}_f$ is a square matrix of second order partial dervatives with the format:\n",
    "\n",
    "$$\n",
    "(H_f)_{i,j} = \\frac{\\partial^{2}f}{\\partial x_i \\partial x_j }\n",
    "$$\n",
    "\n",
    "For the Hessian, we must have a scalar-valued function, $\\bold{f} : \\R^{n} \\to \\R$. It is also possible to compute the Hessian of a vector valued function ($\\bold{f} : \\R^{n} \\to \\R^{m}$), but in this case we would have an array of $m$ Hessian matrices such that $\\bold{H}(\\bold{f})=(\\bold{H}(f_{1}), \\dots, \\bold{H}(f_{m}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Operations\n",
    "### 1. LU Decomposition\n",
    "### 2. Singular Value Decomposition\n",
    "### 3. Cholesky Decomposition\n",
    "### 4. Pseudoinverse "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}